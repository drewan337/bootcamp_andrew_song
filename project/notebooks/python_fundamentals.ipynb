{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bd18d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 03: PYTHON FUNDAMENTALS ===\n",
      "Utility functions defined and ready for use\n",
      "\n",
      "=== STAGE 04: DATA ACQUISITION ===\n",
      "Yahoo Finance error: Too Many Requests. Rate limited. Try after a while.\n",
      "Creating sample data for demonstration...\n",
      "Sample data created\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>150.759075</td>\n",
       "      <td>154.716838</td>\n",
       "      <td>145.251037</td>\n",
       "      <td>150.310957</td>\n",
       "      <td>2422878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>150.422296</td>\n",
       "      <td>155.035412</td>\n",
       "      <td>146.023154</td>\n",
       "      <td>150.959780</td>\n",
       "      <td>3524674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>150.518437</td>\n",
       "      <td>154.752256</td>\n",
       "      <td>147.281992</td>\n",
       "      <td>152.402035</td>\n",
       "      <td>4449872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>151.942619</td>\n",
       "      <td>153.714681</td>\n",
       "      <td>147.101456</td>\n",
       "      <td>153.943102</td>\n",
       "      <td>4947169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>150.764987</td>\n",
       "      <td>154.742616</td>\n",
       "      <td>148.273793</td>\n",
       "      <td>154.313536</td>\n",
       "      <td>3667349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date        open        high         low       close   volume\n",
       "0 2023-01-01  150.759075  154.716838  145.251037  150.310957  2422878\n",
       "1 2023-01-02  150.422296  155.035412  146.023154  150.959780  3524674\n",
       "2 2023-01-03  150.518437  154.752256  147.281992  152.402035  4449872\n",
       "3 2023-01-04  151.942619  153.714681  147.101456  153.943102  4947169\n",
       "4 2023-01-05  150.764987  154.742616  148.273793  154.313536  3667349"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 05: DATA STORAGE ===\n",
      "✓ Raw data saved: data/raw/tsla_raw_20250820-212815.csv\n",
      "\n",
      "=== STAGE 06: DATA PREPROCESSING ===\n",
      "✓ Data preprocessing completed\n",
      "✓ Processed data saved: data/processed/tsla_processed_20250820-212815.parquet\n",
      "\n",
      "Preprocessing results:\n",
      "Rows: 100 → 100\n",
      "Columns: 6 → 17\n",
      "Missing values: 0 → 142\n",
      "\n",
      "Processed data sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>sma_20</th>\n",
       "      <th>sma_50</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>weekly_return</th>\n",
       "      <th>volatility_10</th>\n",
       "      <th>volatility_20</th>\n",
       "      <th>price_sma_10_ratio</th>\n",
       "      <th>price_sma_20_ratio</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>-1.334448</td>\n",
       "      <td>-1.475249</td>\n",
       "      <td>-1.095440</td>\n",
       "      <td>1.246543</td>\n",
       "      <td>-0.420237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.959780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>-1.410667</td>\n",
       "      <td>-1.373906</td>\n",
       "      <td>-0.948322</td>\n",
       "      <td>1.379090</td>\n",
       "      <td>0.583808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.402035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>-1.388909</td>\n",
       "      <td>-1.463982</td>\n",
       "      <td>-0.708463</td>\n",
       "      <td>1.673726</td>\n",
       "      <td>1.426923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.430919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>153.943102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>-1.066589</td>\n",
       "      <td>-1.794049</td>\n",
       "      <td>-0.742862</td>\n",
       "      <td>1.988548</td>\n",
       "      <td>1.880100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.508158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.313536</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>-1.333110</td>\n",
       "      <td>-1.467048</td>\n",
       "      <td>-0.519485</td>\n",
       "      <td>2.064223</td>\n",
       "      <td>0.713825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.441448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.522332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      open      high       low     close    volume  sma_10  \\\n",
       "0 2023-01-01 -1.334448 -1.475249 -1.095440  1.246543 -0.420237     NaN   \n",
       "1 2023-01-02 -1.410667 -1.373906 -0.948322  1.379090  0.583808     NaN   \n",
       "2 2023-01-03 -1.388909 -1.463982 -0.708463  1.673726  1.426923     NaN   \n",
       "3 2023-01-04 -1.066589 -1.794049 -0.742862  1.988548  1.880100     NaN   \n",
       "4 2023-01-05 -1.333110 -1.467048 -0.519485  2.064223  0.713825     NaN   \n",
       "\n",
       "   sma_20  sma_50  daily_return  weekly_return  volatility_10  volatility_20  \\\n",
       "0     NaN     NaN           NaN            NaN            NaN            NaN   \n",
       "1     NaN     NaN      0.705889            NaN            NaN            NaN   \n",
       "2     NaN     NaN      1.430919            NaN            NaN            NaN   \n",
       "3     NaN     NaN      1.508158            NaN            NaN            NaN   \n",
       "4     NaN     NaN      0.441448            NaN            NaN            NaN   \n",
       "\n",
       "   price_sma_10_ratio  price_sma_20_ratio  next_day_close  target  \n",
       "0                 NaN                 NaN      150.959780       1  \n",
       "1                 NaN                 NaN      152.402035       1  \n",
       "2                 NaN                 NaN      153.943102       1  \n",
       "3                 NaN                 NaN      154.313536       1  \n",
       "4                 NaN                 NaN      155.522332       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target distribution:\n",
      "Up days: 48 (48.0%)\n",
      "Down days: 52 (52.0%)\n",
      "\n",
      "=== ALL STAGES COMPLETED ===\n",
      "✓ Python Fundamentals - Utility functions implemented\n",
      "✓ Data Acquisition - TSLA data retrieved\n",
      "✓ Data Storage - Raw/processed data saved\n",
      "✓ Data Preprocessing - Data cleaned and features engineered\n",
      "\n",
      "Data ready for modeling in: data/processed/tsla_processed_20250820-212815.parquet\n"
     ]
    }
   ],
   "source": [
    "# TSLA Stock Analysis - Stages 03-06\n",
    "# Python Fundamentals, Data Acquisition, Storage, and Preprocessing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pathlib\n",
    "import datetime as dt\n",
    "from typing import Union, Dict, Any, List\n",
    "\n",
    "# ==================== STAGE 03: PYTHON FUNDAMENTALS ====================\n",
    "print(\"=== STAGE 03: PYTHON FUNDAMENTALS ===\")\n",
    "\n",
    "# Utility Functions (reusable across stages)\n",
    "def ts() -> str:\n",
    "    \"\"\"Generate timestamp string for consistent filenames.\"\"\"\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def validate_data(df: pd.DataFrame, required_columns: list) -> Dict[str, Any]:\n",
    "    \"\"\"Validate DataFrame structure and data quality.\"\"\"\n",
    "    validation = {\n",
    "        'missing_columns': [col for col in required_columns if col not in df.columns],\n",
    "        'shape': df.shape,\n",
    "        'na_total': int(df.isna().sum().sum()),\n",
    "        'na_by_column': df.isna().sum().to_dict(),\n",
    "        'dtypes': df.dtypes.to_dict()\n",
    "    }\n",
    "    return validation\n",
    "\n",
    "def detect_format(path: Union[str, pathlib.Path]) -> str:\n",
    "    \"\"\"Detect file format from extension.\"\"\"\n",
    "    path_str = str(path).lower()\n",
    "    if path_str.endswith('.csv'):\n",
    "        return 'csv'\n",
    "    if any(path_str.endswith(ext) for ext in ['.parquet', '.pq', '.parq']):\n",
    "        return 'parquet'\n",
    "    raise ValueError(f'Unsupported format: {path_str}')\n",
    "\n",
    "def read_df(path: Union[str, pathlib.Path]) -> pd.DataFrame:\n",
    "    \"\"\"Read DataFrame from file with automatic format detection.\"\"\"\n",
    "    path_obj = pathlib.Path(path)\n",
    "    fmt = detect_format(path_obj)\n",
    "    \n",
    "    if fmt == 'csv':\n",
    "        try:\n",
    "            sample = pd.read_csv(path_obj, nrows=5)\n",
    "            date_cols = [col for col in sample.columns if 'date' in col.lower()]\n",
    "            if date_cols:\n",
    "                return pd.read_csv(path_obj, parse_dates=date_cols)\n",
    "            return pd.read_csv(path_obj)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'Failed to read CSV: {e}')\n",
    "    \n",
    "    elif fmt == 'parquet':\n",
    "        try:\n",
    "            return pd.read_parquet(path_obj)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Parquet engine not available. Install pyarrow or fastparquet.')\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: Union[str, pathlib.Path]) -> pathlib.Path:\n",
    "    \"\"\"Write DataFrame to file with automatic format detection.\"\"\"\n",
    "    path_obj = pathlib.Path(path)\n",
    "    path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fmt = detect_format(path_obj)\n",
    "    \n",
    "    if fmt == 'csv':\n",
    "        df.to_csv(path_obj, index=False)\n",
    "    elif fmt == 'parquet':\n",
    "        try:\n",
    "            df.to_parquet(path_obj)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('Parquet engine not available. Install pyarrow or fastparquet.')\n",
    "    \n",
    "    return path_obj\n",
    "\n",
    "def calculate_technical_indicators(df: pd.DataFrame, price_col: str = 'close') -> pd.DataFrame:\n",
    "    \"\"\"Calculate common technical indicators for stock data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Simple Moving Averages\n",
    "    df['sma_10'] = df[price_col].rolling(window=10).mean()\n",
    "    df['sma_20'] = df[price_col].rolling(window=20).mean()\n",
    "    df['sma_50'] = df[price_col].rolling(window=50).mean()\n",
    "    \n",
    "    # Price Returns\n",
    "    df['daily_return'] = df[price_col].pct_change()\n",
    "    df['weekly_return'] = df[price_col].pct_change(5)\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility_10'] = df['daily_return'].rolling(window=10).std()\n",
    "    df['volatility_20'] = df['daily_return'].rolling(window=20).std()\n",
    "    \n",
    "    # Price vs SMA ratios\n",
    "    df['price_sma_10_ratio'] = df[price_col] / df['sma_10']\n",
    "    df['price_sma_20_ratio'] = df[price_col] / df['sma_20']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ensure_data_directories():\n",
    "    \"\"\"Ensure all data directories exist.\"\"\"\n",
    "    directories = ['data/raw', 'data/processed']\n",
    "    for dir_path in directories:\n",
    "        pathlib.Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_latest_file(directory: Union[str, pathlib.Path], pattern: str = \"*\") -> pathlib.Path:\n",
    "    \"\"\"Get the most recent file in a directory.\"\"\"\n",
    "    dir_path = pathlib.Path(directory)\n",
    "    files = list(dir_path.glob(pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found in {directory} matching {pattern}\")\n",
    "    return max(files, key=lambda x: x.stat().st_mtime)\n",
    "\n",
    "print(\"Utility functions defined and ready for use\\n\")\n",
    "\n",
    "# ==================== STAGE 04: DATA ACQUISITION ====================\n",
    "print(\"=== STAGE 04: DATA ACQUISITION ===\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "SYMBOL = os.getenv('YFINANCE_SYMBOL', 'TSLA')\n",
    "ALPHAVANTAGE_API_KEY = os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "\n",
    "def fetch_from_yfinance(symbol, period='2y', interval='1d'):\n",
    "    \"\"\"Fetch data from Yahoo Finance\"\"\"\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(period=period, interval=interval)\n",
    "        df = df.reset_index()\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Yahoo Finance error: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_from_alphavantage(symbol, api_key):\n",
    "    \"\"\"Fetch data from Alpha Vantage API\"\"\"\n",
    "    if not api_key:\n",
    "        return None\n",
    "    \n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    params = {\n",
    "        'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
    "        'symbol': symbol,\n",
    "        'outputsize': 'compact',\n",
    "        'apikey': api_key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        time_series = data.get('Time Series (Daily)', {})\n",
    "        df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "        df = df.reset_index().rename(columns={'index': 'date'})\n",
    "        \n",
    "        column_mapping = {\n",
    "            '1. open': 'open', '2. high': 'high', '3. low': 'low',\n",
    "            '4. close': 'close', '5. adjusted close': 'adj_close',\n",
    "            '6. volume': 'volume'\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        numeric_cols = ['open', 'high', 'low', 'close', 'adj_close', 'volume']\n",
    "        for col in numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Alpha Vantage API error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Acquire data\n",
    "df = fetch_from_yfinance(SYMBOL)\n",
    "\n",
    "if df is None or df.empty:\n",
    "    df = fetch_from_alphavantage(SYMBOL, ALPHAVANTAGE_API_KEY)\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    validation = validate_data(df, ['date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    print(f\"✓ Data acquired: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\"   Missing values: {validation['na_total']}\")\n",
    "else:\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    dates = pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'open': 150 + np.random.randn(100).cumsum(),\n",
    "        'high': 155 + np.random.randn(100).cumsum(),\n",
    "        'low': 145 + np.random.randn(100).cumsum(),\n",
    "        'close': 150 + np.random.randn(100).cumsum(),\n",
    "        'volume': np.random.randint(1000000, 5000000, 100)\n",
    "    })\n",
    "    print(\"Sample data created\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# ==================== STAGE 05: DATA STORAGE ====================\n",
    "print(\"\\n=== STAGE 05: DATA STORAGE ===\")\n",
    "\n",
    "# Ensure directories exist\n",
    "ensure_data_directories()\n",
    "RAW_DIR = os.getenv('DATA_DIR_RAW', 'data/raw')\n",
    "PROCESSED_DIR = os.getenv('DATA_DIR_PROCESSED', 'data/processed')\n",
    "\n",
    "# Save raw data\n",
    "raw_filename = f\"tsla_raw_{ts()}.csv\"\n",
    "raw_path = f\"{RAW_DIR}/{raw_filename}\"\n",
    "write_df(df, raw_path)\n",
    "\n",
    "print(f\"✓ Raw data saved: {raw_path}\")\n",
    "\n",
    "# ==================== STAGE 06: DATA PREPROCESSING ====================\n",
    "print(\"\\n=== STAGE 06: DATA PREPROCESSING ===\")\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self):\n",
    "        self.required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \n",
    "    def drop_missing_columns(self, df: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:\n",
    "        missing_ratio = df.isnull().mean()\n",
    "        columns_to_drop = missing_ratio[missing_ratio > threshold].index.tolist()\n",
    "        return df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    def fill_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_clean = df.copy()\n",
    "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if df_clean[col].isnull().any():\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "        \n",
    "        if 'date' in df_clean.columns:\n",
    "            df_clean = df_clean.sort_values('date').ffill()\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return calculate_technical_indicators(df)\n",
    "    \n",
    "    def create_target_variable(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_target = df.copy()\n",
    "        df_target['next_day_close'] = df_target['close'].shift(-1)\n",
    "        df_target['target'] = (df_target['next_day_close'] > df_target['close']).astype(int)\n",
    "        return df_target.dropna(subset=['target'])\n",
    "    \n",
    "    def normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['target', 'next_day_close']]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        df_normalized = df.copy()\n",
    "        df_normalized[numeric_cols] = scaler.fit_transform(df_normalized[numeric_cols])\n",
    "        \n",
    "        return df_normalized, scaler\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        results = {}\n",
    "        initial_validation = validate_data(df, self.required_columns)\n",
    "        results['initial_validation'] = initial_validation\n",
    "        \n",
    "        df_clean = self.drop_missing_columns(df)\n",
    "        df_clean = self.fill_missing_values(df_clean)\n",
    "        df_clean = self.add_technical_indicators(df_clean)\n",
    "        df_clean = self.create_target_variable(df_clean)\n",
    "        df_clean, scaler = self.normalize_data(df_clean)\n",
    "        \n",
    "        results['final_validation'] = validate_data(df_clean, [])\n",
    "        results['scaler'] = scaler\n",
    "        \n",
    "        return df_clean, results\n",
    "\n",
    "# Preprocess the data\n",
    "cleaner = DataCleaner()\n",
    "df_clean, results = cleaner.clean_data(df)\n",
    "\n",
    "print(\"✓ Data preprocessing completed\")\n",
    "\n",
    "# Save processed data\n",
    "processed_filename = f\"tsla_processed_{ts()}.parquet\"\n",
    "processed_path = f\"{PROCESSED_DIR}/{processed_filename}\"\n",
    "write_df(df_clean, processed_path)\n",
    "\n",
    "print(f\"✓ Processed data saved: {processed_path}\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nPreprocessing results:\")\n",
    "print(f\"Rows: {results['initial_validation']['shape'][0]} → {results['final_validation']['shape'][0]}\")\n",
    "print(f\"Columns: {results['initial_validation']['shape'][1]} → {results['final_validation']['shape'][1]}\")\n",
    "print(f\"Missing values: {results['initial_validation']['na_total']} → {results['final_validation']['na_total']}\")\n",
    "\n",
    "print(\"\\nProcessed data sample:\")\n",
    "display(df_clean.head())\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "target_counts = df_clean['target'].value_counts()\n",
    "print(f\"Up days: {target_counts.get(1, 0)} ({target_counts.get(1, 0)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Down days: {target_counts.get(0, 0)} ({target_counts.get(0, 0)/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== ALL STAGES COMPLETED ===\")\n",
    "print(\"✓ Python Fundamentals - Utility functions implemented\")\n",
    "print(\"✓ Data Acquisition - TSLA data retrieved\")\n",
    "print(\"✓ Data Storage - Raw/processed data saved\")\n",
    "print(\"✓ Data Preprocessing - Data cleaned and features engineered\")\n",
    "print(f\"\\nData ready for modeling in: {processed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
